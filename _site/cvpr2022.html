<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Computer Vision in the Built Environment</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Computer Vision in the Built Environment" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CV4AEC @ CVPR 2022 (19 June 2022)" />
<meta property="og:description" content="CV4AEC @ CVPR 2022 (19 June 2022)" />
<link rel="canonical" href="http://localhost:4000/cvpr2022.html" />
<meta property="og:url" content="http://localhost:4000/cvpr2022.html" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Computer Vision in the Built Environment" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"CV4AEC @ CVPR 2022 (19 June 2022)","headline":"Computer Vision in the Built Environment","url":"http://localhost:4000/cvpr2022.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<link rel="apple-touch-icon" sizes="180x180" href="assets/imgs/favicon/apple-touch-icon.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="assets/imgs/favicon/favicon-32x32.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="assets/imgs/favicon/favicon-16x16.jpg">
<link rel="manifest" href="assets/imgs/favicon/site.webmanifest">

<!--Set all links to open in new tab by default-->
<base target="_blank">

<!-- end custom head snippets -->


  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Computer Vision in the Built Environment</h1>
      <h2 class="project-tagline">CV4AEC @ CVPR 2022 (19 June 2022)</h2>
<!--      -->
      

      <a href="#news" class="btn" target="_self">News</a>
      <a href="#dates" class="btn" target="_self">Important Dates</a>
      <a href="#schedule" class="btn" target="_self">Schedule</a>
      <a href="#speakers" class="btn" target="_self">Keynote Speakers</a>
      <a href="#papers" class="btn" target="_self">Call for Papers</a>
      <a href="#challenge" class="btn" target="_self">Challenge</a>
      <a href="#organizers" class="btn" target="_self">Organizers</a>
      <br>
      <a href="/index" class="btn" target="_self" style="width:150px">Latest</a>
      <a href="/cvpr2024" class="btn" target="_self" style="width:150px">CVPR 2024</a>
      <a href="/cvpr2023" class="btn" target="_self" style="width:150px">CVPR 2023</a>
      <a href="/cvpr2022" class="btn" target="_self" style="width:150px">CVPR 2022</a>
      <a href="/cvpr2021" class="btn" target="_self" style="width:150px">CVPR 2021</a>
      
    </header>

    <script src="assets/js/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop({
      diameter: 56,
      backgroundColor: '#4196ff',
      textColor: '#fff',
    })</script>

    <main id="content" class="main-content" role="main">
      <p class="text-center"><img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> Welcome to the <strong>2<sup>nd</sup> Workshop and Challenge on
Computer Vision In The Built Environment For The Design, Construction and Operation of Buildings</strong> organized at <img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> <a href="https://cvpr.thecvf.com/"><img class="rounded-rect" src="assets/imgs/cvpr2022.png" width="400px" alt="cvpr2022"></a></p>

<p>Building on the success of the 1st workshop, the 2nd Workshop on Computer Vision in the Built Environment continous on connecting the domains of Architecture, Engineering, and Construction (AEC) with that of Computer Vision by establishing a common ground of interaction and identify shared research interests. . Specifically, this workshop focuses on the as-is semantic status of built environments and the changes that take place within them over time. These topics will be presented from the dual lens of Computer Vision and AEC-FM, highlighting the limitations and bottlenecks related to developing applications for this specific domain. The objective is for attendees to learn more about AEC-FM and the variety of real-world problems that, if solved, could have a tangible impact on this multi trillion dollar industry as well as the overall quality of life across the globe.</p>

<p>The workshop will begin by establishing ways to acquire the as-is status of a space in a granular and hierarchical way - some of the speakers are experts in acquiring the spatial layout whereas others focus on object categories and their attributes. Building on this static scene understanding, we introduce the impact of time, as change that is either explicitly observed (a human interacting with an object) or implicitly inferred (capturing the as-is status of a scene in different timestamps). The combination of the static and dynamic understanding of 3D scenes is at the core of AEC-FM industry and currently missing. One example is that architects typically design living spaces without any feedback from their previous designs. Another example is that 5-12% (this percentage corresponds annually to billions of dollars in the US alone) of non-estimated construction cost is due to rework that originates from misinterpretation of design documents and the dynamically changing environment of construction sites.</p>

<p>To further establish connections between the two domains and identify what we can do right now and what is still hard to solve, we will host the <strong>2nd International Scan-to-BIM competition</strong> targeted on acquiring the semantic as-is status of buildings given their 3D point clouds. Specifically, we will focus on the tasks of floorplan reconstruction and 3D building model reconstruction and present appropriate interdisciplinary metrics for solving them. Last year we observed that a large gap remains before these problems can be considered solved and actually meet the needs of practitioners. We regard this workshop as the ideal environment for understanding the challenges and steps forward given that it provides convergence between the research and practical communities from multiple disciplines.</p>

<p>The workshop will therefore consist of two parts: invited <a href="#speakers" target="_self">keynote talks</a> and a  <a href="#challenge" target="_self">Scan-to-BIM challenge</a>.</p>

<hr>

<h2 id="dates">
<img class="emoji" title=":hourglass_flowing_sand:" alt=":hourglass_flowing_sand:" src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png" height="20" width="20"> <strong>Important Dates</strong>
</h2>
<blockquote>
  <p><strong>NOTE</strong>: The submission/release times are <strong>00:00:00 UTC</strong> on the specified date.</p>
</blockquote>

<ul>
  <li>
<strong>01 May 2022 —</strong> Evaluation server open to evaluate test submissions</li>
  <li>
<strong>12 Jun 2022 (11:59PM PDT) —</strong> Challenge submission deadline</li>
  <li>
<strong>15 Jun 2022 —</strong> Notification to participants</li>
  <li>
<strong>19 Jun 2022 —</strong> CV4AEC Workshop @ CVPR 2022</li>
</ul>

<hr>

<h2 id="schedule">
<img class="emoji" title=":calendar:" alt=":calendar:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c6.png" height="20" width="20"> <strong>Schedule</strong>
</h2>
<p>The workshop took place on <strong>19 June 2022</strong> from <strong>09:00 - 18:00</strong>. The recording can be found <a href="https://oregonstate.app.box.com/s/q0j9uzpl4zm3xwnqk87in3z6ezrlrlqi">here</a>.</p>

<blockquote>
  <p><strong>NOTE</strong>: Times are shown in <strong>Central Standard Time</strong>. Please take this into account if joining the workshop virtually.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Time (PDT)</th>
      <th>Duration</th>
      <th>Event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>09:00 - 09:30</td>
      <td>30 mins</td>
      <td>Introduction To The Workshop &amp; Challenge</td>
    </tr>
    <tr>
      <td>09:30 - 10:00</td>
      <td>30 mins</td>
      <td>
<strong>Burcu Akinci</strong> – Lessons learned from decades of research in utilizing computer vision to support construction and infrastructure management</td>
    </tr>
    <tr>
      <td>10:00 - 10:30</td>
      <td>30 mins</td>
      <td>
<strong>Angela Dai</strong> – Learning from Synthetic 3D Priors for Real-World 3D Perception</td>
    </tr>
    <tr>
      <td>10:30 - 11:15</td>
      <td>30 mins</td>
      <td>Winner Presentations, 2D Floorplan Reconstruction</td>
    </tr>
    <tr>
      <td>11:15 - 11:30</td>
      <td>15 mins</td>
      <td><em>Coffee Break</em></td>
    </tr>
    <tr>
      <td>11:30 - 12:00</td>
      <td>30 mins</td>
      <td>
<strong>Shirley Dyke</strong> – Applying Machine Learning to Support Disaster Reconnaissance</td>
    </tr>
    <tr>
      <td>12:00 - 12:30</td>
      <td>30 mins</td>
      <td>
<strong>Siyu Tang</strong>  – Human Motion Capture and Synthesis in 3D Scenes</td>
    </tr>
    <tr>
      <td>12:30 - 13:15</td>
      <td>45 mins</td>
      <td>Winner Presentations, 3D Building Model Reconstruction</td>
    </tr>
    <tr>
      <td>13:15 - 14:15</td>
      <td>60 mins</td>
      <td><em>Lunch Break</em></td>
    </tr>
    <tr>
      <td>14:45 - 15:00</td>
      <td>45 mins</td>
      <td>Community Engagement</td>
    </tr>
    <tr>
      <td>15:00 - 15:30</td>
      <td>30 mins</td>
      <td>
<strong>Chen Fueng</strong> – Weakly and Self Supervised Robot Perception: from Scene Understanding to Mobile Construction in AEC</td>
    </tr>
    <tr>
      <td>15:30 - 16:00</td>
      <td>30 mins</td>
      <td>
<strong>Thomas Funkhouser</strong> – Neural Scene Representations in Urban Environments</td>
    </tr>
    <tr>
      <td>16:00 - 16:30</td>
      <td>30 mins</td>
      <td>
<strong>Federico Tombari</strong> – 3D scene understanding with scene graphs and self-supervision for AR and indoor design</td>
    </tr>
    <tr>
      <td>16:30 - 17:00</td>
      <td>30 mins</td>
      <td><em>Coffee Break</em></td>
    </tr>
    <tr>
      <td>17:00 - 17:45</td>
      <td>45 mins</td>
      <td><em>Panel Discussion</em></td>
    </tr>
    <tr>
      <td>17:45 - 18:00</td>
      <td>15 mins</td>
      <td><em>Concluding Remarks</em></td>
    </tr>
  </tbody>
</table>

<hr>

<h2 id="speakers">
<img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> <strong>Keynote Speakers</strong>
</h2>

<div class="container">

<figure>
    <a href="https://www.cmu.edu/cee/people/faculty/akinci.html">
    <img class="img-author" src="assets/imgs/authors/cvpr2022/burcuakinci.jpg" alt="Burcu Akinci"></a>
    <b><br><a href="https://www.cmu.edu/cee/people/faculty/akinci.html">Burcu Akinci</a>
    <br>Professor, CEE <br>CMU</b>
</figure>

<figure>
    <a href="https://www.3dunderstanding.org/team.html">
    <img class="img-author" src="assets/imgs/authors/cvpr2022/angeladai.jpeg" alt="Angela Dai"></a>
    <b><br><a href="https://www.3dunderstanding.org/team.html">Angela Dai</a>
    <br>Professor, CS <br>TU MUnich</b>
</figure>

<figure>
    <a href="https://engineering.purdue.edu/ME/People/ptProfile?resource_id=57291">
    <img class="img-author" src="assets/imgs/authors/cvpr2022/shirleydyke.jpg" alt="Shirley J. Dyke"></a>
    <b><br><a href="https://engineering.purdue.edu/ME/People/ptProfile?resource_id=57291">Shirley J. Dyke</a>
    <br>Professor, ME &amp; CEE <br>Purdue</b>
</figure>

<figure>
    <a href="https://engineering.nyu.edu/faculty/chen-feng">
    <img class="img-author" src="assets/imgs/authors/cvpr2022/chenfeng.jpeg" alt="Chen Feng"> </a>
    <b><br><a href="https://engineering.nyu.edu/faculty/chen-feng">Chen Feng</a>
    <br>Professor, ME &amp; CEE <br>NYU</b>
</figure>

<figure>
    <a href="https://www.cs.princeton.edu/~funk/">
    <img class="img-author" src="assets/imgs/authors/cvpr2022/tomfunkhouser.jpeg" alt="Thomas Funkhouser"></a>
    <b><br><a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
    <br>Senior Research Scientist<br>Google</b>
</figure>

<figure>
    <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">
    <img class="img-author" src="assets/imgs/authors/cvpr2022/siyutang.jpeg" alt="Siyu Tang"></a>
    <b><br><a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>
    <br>Professor, CS <br>ETHZ</b>
</figure>

<figure>
    <a href="https://federicotombari.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2022/federicotombari.jpeg" alt="Federico Tombari"></a>
    <b><br><a href="https://federicotombari.github.io/">Federico Tombari</a>
    <br>Senior Staff Research Scientist and Manager<br>Google</b>
</figure>

</div>

<p><a href="https://www.cmu.edu/cee/people/faculty/akinci.html"><strong>Burcu Akinci</strong></a>
is Paul Christiano Professor of Civil &amp; Environmental Engineering at Carnegie Mellon University and a member of the National Academies of Construction. She was also former Associate Dean for Research for the College of Engineering and Director of Engineering Research Accelerator at Carnegie Mellon. She earned a bachelor’s degree in civil engineering from the Middle East Technical University (Ankara, Turkey), MBA from Bilkent University (Ankara, Turkey), and Master’s and PhD degrees in Civil and Environmental Engineering with a specialization in Construction Engineering and Management from Stanford University. Dr. Akinci’s research focuses on investigating utilization and integration of building information models with data capture technologies, such as 3D imaging and embedded sensors, to create digital twins of construction projects and infrastructure operations, and develop approaches to support proactive and predictive operations and management.</p>

<p><a href="https://www.3dunderstanding.org/team.html"><strong>Angela Dai</strong></a>
is an Assistant Professor at the Technical University of Munich. Her research focuses on understanding how the 3D world around us can be modeled and semantically understood, leveraging generative deep learning towards enabling understanding and interaction with real-world 3D/4D scenes for content creation and virtual or robotic agents. Previously, she received her PhD in computer science from Stanford in 2018 and her BSE in computer science from Princeton in 2013. Her research has been recognized through a ZDB Junior Research Group Award, an ACM SIGGRAPH Outstanding Doctoral Dissertation Honorable Mention, as well as a Stanford Graduate Fellowship.</p>

<p><a href="https://engineering.purdue.edu/ME/People/ptProfile?resource_id=57291"><strong>Shirley J. Dyke</strong></a>
holds a joint appointment in Mechanical Engineering and Civil Engineering at Purdue University. She is the Director of Purdue’s Intelligent Infrastructure Systems Lab and the Director of the NASA funded Resilient ExtraTerrestrial Habitat Institute. Dyke is the Editor-in-Chief of the journal Engineering Structures. Her research focuses on “intelligent” structures, and her innovations encompass structural health monitoring and machine learning for structural damage assessment and reconnaissance support. She holds a B.S. in Aeronautical and Astronautical Engineering from the University of Illinois, Champaign-Urbana in 1991 and a Ph.D. in Civil Engineering from the University of Notre Dame in 1996. She was awarded the Presidential Early Career Award for Scientists and Engineers from NSF (1998), the International Association on Structural Safety and Reliability Junior Research Award (2001) and the ANCRiSST Young Investigator Award (2006).</p>

<p><a href="%22https://engineering.nyu.edu/faculty/chen-feng%22"><strong>Chen Feng</strong></a> 
is an assistant professor at NYU, appointed across departments including civil and mechanical engineering, and computer science. His lab AI4CE (pronounced as A-I-force) aims to advance robot vision and machine learning through multidisciplinary use-inspired research that originates from civil/mechanical engineering domains. Before NYU, Chen was a research scientist in the computer vision group at Mitsubishi Electric Research Labs (MERL) in Cambridge, MA, focusing on localization, mapping, and deep learning for self-driving cars and robotics. Chen holds a Bachelor’s degree in geospatial engineering from Wuhan University in China, and a master’s degree in electrical engineering and a Ph.D. in civil engineering, both from the University of Michigan at Ann Arbor.</p>

<p><a href="https://www.cs.princeton.edu/~funk/"><strong>Thomas Funkhouser</strong></a> 
is a Senior Research Scientist in Google and the David M. Siegel Professor, Emeritus, at the CS Department, Princeton University. Thomas joined Princeton University in 1998 as an assistant professor. He became an associate professor in 2003 and a full professor in 2009. Before coming to Princeton, he worked for four years on the technical staff at Bell Laboratories. He holds a Ph.D. in computer science from the University of California, Berkeley (1993), a Master’s in computer science from UCLA, and a Bachelor’s in biological sciences from Stanford. Among Professor Funkhouser’s honors and awards are the ACM SIGGRAPH Computer Graphics Achievement Award (2014), Sloan Foundation Fellowship (1999), and National Science Foundation Career Award (2000).</p>

<p><a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html"><strong>Siyu Tang</strong></a>
is an assistant professor at ETH Zürich in the Department of Computer Science since January 2020. She received an early career research grant to start her own research group at the Max Planck Institute for Intelligent Systems in November 2017. She was a postdoctoral researcher in the same institute, advised by Dr. Michael Black. She finished her PhD at the Max Planck Institute for Informatics and Saarland University in 2017, under the supervision of Professor Bernt Schiele. Before that, she received her Master’s degree in Media Informatics at RWTH Aachen University, advised by Prof. Bastian Leibe and her Bachelor degree in Computer Science at Zhejiang University, China. She has received several awards for her research, including the Best Paper Award at BMVC 2012 and 3DV 2020, Best Paper Award Finalist at CVPR 2021, an ELLIS PhD Award and a DAGM-MVTec Dissertation Award.</p>

<p><a href="https://federicotombari.github.io/"><strong>Federico Tombari</strong></a>
is Senior Staff Research Scientist and Manager at Google where he leads an applied research team in computer vision and ML. He is also a Lecturer (PrivatDozent) at the Technical University of Munich (TUM). He has 230+ peer-reviewed publications in CV/ML and applications to robotics, autonomous driving, healthcare and augmented reality. He got his PhD from the University of Bologna and his Habilitation from TUM. In 2018 he was co-founder and managing director of Pointu3D, a startup then acquired by Google. He regularly serves as Chair and Associate Editor for international conferences and journals (RA-L, ECCV18/22, IROS20/21/22, ICRA20/22, 3DV19/20/21 among others). He was the recipient of two Google Faculty Research Awards, one Amazon Research Award, 5 Outstanding Reviewer Awards (3x CVPR, ICCV21, NeurIPS 2021).</p>

<h2 id="challenge">
<img class="emoji" title=":checkered_flag:" alt=":checkered_flag:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c1.png" height="20" width="20"> <strong>Challenge</strong>
</h2>
<p>The workshop will host the 2nd International Scan-to-BIM challenge. The challenge will include the following tasks:</p>

<p>I. 2D Floorplan Reconstruction <br>
II. 3D Building Model Reconstruction</p>

<p class="text-center"><strong>[<a href="https://github.com/GradientSpaces/cv4aec-challenge">GitHub</a>] — [<a href="https://codalab.lisn.upsaclay.fr/competitions/18523">2D Challenge</a>] — [<a href="https://codalab.lisn.upsaclay.fr/competitions/18526">3D Challenge</a>]</strong></p>

<h3 id="2d-floor-plan-reconstruction">2D Floor Plan Reconstruction</h3>

<p>The 2D Floorplan Reconstruction challenge contains a total of 31 buildings with multiple floors each and dozens of rooms on each floor. Of which, 20 buildings are designated as the training set, with a total of 49 point clouds. The validation and testing sets contain 5.5 buildings with 21 point clouds each. For each model, there is an aligned point cloud in LAZ format. For the training and validation sets, a corresponding floorplan aligned with the coordinate system of the point cloud is also provided. The challenge data and evaluation code can be found in this <strong><a href="https://github.com/GradientSpaces/cv4aec-challenge">Github repository</a></strong>. The submission should be made in the same JSON format as in the provided ground truth. We include metrics to evaluate the reconstruction of the walls, doors, and columns, as well as floor area in 2D :</p>

<ol>
  <li>
    <p><strong>Geometric Metrics</strong> <br>
 a. <em>IoU</em> of each room (a room is defined as a completely separated area with walls and doors). <br>
 b. <em>Accuracy of endpoints</em> : Precision/Recall at 3 different thresholds: 5cm, 10cm and 20cm, as well as the F-measure at each threshold will be evaluated in the coordinate system of the point cloud. The provided endpoints will be matched with the Hungarian algorithm to the point cloud, and every point that is within a certain threshold will be determined as a match. <br>
 c. <em>Orientation</em> For each matched line between the ground truth, we will compute the cosine similarity metric between them as the normalized dot product. If a line is not matched with ground truth, the cosine metric will be zero. Finally, the metric will be averaged over all the ground truth lines.</p>
  </li>
  <li>
    <p><strong>Topological Metrics</strong> <br>
 a. <em><a href="https://ieeexplore.ieee.org/document/5539950">Warping error</a></em> : The warping error will first warp the predicted floorplan to the ground truth with a homotopic deformation, and then compute the pixels that cannot match after the deformation. <br>
 b. <strong><em>Betti number error</em></strong> : The Betti number error will compare the Betti numbers between the prediction and the ground truth and output the absolute value of the difference.</p>
  </li>
</ol>

<h3 id="3d-building-model-reconstruction">3D Building Model Reconstruction</h3>

<p>The training data consists of 11 floors from 7 buildings. For each model, there is an aligned point cloud in LAZ format. The 3D building coordinates for walls, columns and doors are presented in 3 separate JSON files. We focus on the reconstruction of walls, columns, and doors. The challenge data and evaluation code can be found in this <strong><a href="https://github.com/GradientSpaces/cv4aec-challenge">Github repository</a></strong>. The submission should be made in the same JSON format as in the provided ground truth. We evaluate the submissions on a variety of metrics :</p>

<ol>
  <li>
<strong>3D IoU</strong> of the 3D bounding box of each wall</li>
  <li>
<strong>Accuracy of the endpoints</strong> : Precision/Recall at 3 different thresholds: 5cm, 10cm and 20cm, as well as F-measure will be evaluated in the coordinate system of the point cloud. The provided endpoints will be matched with the Hungarian algorithm to the point cloud, and every point that is within a certain threshold will be determined as a match. We evaluate per each of the three semantic types (i.e., wall, column, door).</li>
</ol>

<blockquote>
  <p>We decided to NOT provide proprietary formats such as Autodesk Revit files since there does not exist good open-source APIs to read them. The reason we decided not to provide an open format such as DXF is because DXF exports have arbitrary designations of conjunctions of walls, i.e. the corner will belong to only one of the walls in the DXF files, and the designation which corner belongs to which wall is arbitrary.
In our JSON format, we provide walls as middle lines + thickness. The middle lines will connect to each other at corners. Hence there is no ambiguity on which part of the corner belongs to which wall.
We would like to note that ALL the submissions <strong>need to be constructed automatically</strong>. Manual reconstructions are against the spirit of this challenge and will not be allowed.</p>
</blockquote>

<hr>

<h2 id="winners">
<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> <strong>Challenge Winners</strong>
</h2>

<h3 id="2d-floor-plan-reconstruction-1">2D Floor Plan Reconstruction</h3>

<table>
  <thead>
    <tr>
      <th>Team</th>
      <th>Precision (5cm)</th>
      <th>Precision (10cm)</th>
      <th>Precision (20cm)</th>
      <th>Recall(5cm)</th>
      <th>Recall(10cm)</th>
      <th>Recall (20cm)</th>
      <th>IoU</th>
      <th>Warping Error</th>
      <th>Betting Error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Seg2Plan</td>
      <td>0.052</td>
      <td>0.203</td>
      <td>0.335</td>
      <td>0.015</td>
      <td>0.065</td>
      <td>0.114</td>
      <td>0.657</td>
      <td>0.249</td>
      <td>1.076</td>
    </tr>
    <tr>
      <td>S2FP</td>
      <td>0.020</td>
      <td>0.085</td>
      <td>0.146</td>
      <td>0.048</td>
      <td>0.220</td>
      <td>0.375</td>
      <td>0.517</td>
      <td>0.188</td>
      <td>1.140</td>
    </tr>
    <tr>
      <td>FLKPP</td>
      <td>0.016</td>
      <td>0.068</td>
      <td>0.132</td>
      <td>0.032</td>
      <td>0.129</td>
      <td>0.253</td>
      <td>0.374</td>
      <td>0.258</td>
      <td>1.128</td>
    </tr>
    <tr>
      <td>VecIM</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.731</td>
      <td>1.646</td>
    </tr>
  </tbody>
</table>

<h3 id="3d-building-model-reconstruction-1">3D Building Model Reconstruction</h3>

<table>
  <thead>
    <tr>
      <th>Team</th>
      <th>Average IoU</th>
      <th>Columns IoU</th>
      <th>Doors IoU</th>
      <th>Walls IoU</th>
      <th>5cm Average F1</th>
      <th>10cm Average F1</th>
      <th>20cm Average F1</th>
      <th>10cm Columns F1</th>
      <th>10cm Doors F1</th>
      <th>10cm Walls F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Seg2BIM</td>
      <td>0.309</td>
      <td>0.470</td>
      <td>0.260</td>
      <td>0.266</td>
      <td>0.417</td>
      <td>0.515</td>
      <td>0.577</td>
      <td>0.618</td>
      <td>0.494</td>
      <td>0.477</td>
    </tr>
    <tr>
      <td>FLKPP</td>
      <td>0.231</td>
      <td>0.372</td>
      <td>0.230</td>
      <td>0.152</td>
      <td>0.316</td>
      <td>0.454</td>
      <td>0.584</td>
      <td>0.608</td>
      <td>0.367</td>
      <td>0.452</td>
    </tr>
    <tr>
      <td>PointToBIM</td>
      <td>0.170</td>
      <td>0.396</td>
      <td>0.061</td>
      <td>0.150</td>
      <td>0.276</td>
      <td>0.366</td>
      <td>0.448</td>
      <td>0.633</td>
      <td>0.165</td>
      <td>0.415</td>
    </tr>
    <tr>
      <td>BoxDetector</td>
      <td>0.024</td>
      <td>0.038</td>
      <td>0.006</td>
      <td>0.033</td>
      <td>0.109</td>
      <td>0.171</td>
      <td>0.258</td>
      <td>0.167</td>
      <td>0.144</td>
      <td>0.197</td>
    </tr>
  </tbody>
</table>

<h3 id="teams"><strong>Teams</strong></h3>
<ul>
  <li>
<strong>Seg2Plan</strong>: Shengtao Li, Gaoming Fan, Han Huang, Hairong Luo, Ge Gao, Ming Gu; School of software/BNRist, Tsinghua University</li>
  <li>
<strong>S2FP</strong>: Ekin Celikkan, Theodora Kontogianni, Francis Engelmann; RWTH Aachen University (Computer Vision Group), ETH Zurich (AI Center)</li>
  <li>
<strong>FLKPP</strong>: Yijie Wu, Maosu Li, Fan Xue; The University of Hong Kong</li>
  <li>
<strong>VecIM</strong>: Paul Liu; Institute of Automation, Chinese Academy of Sciences</li>
  <li>
<strong>Seg2BIM</strong>: Han Huang, Gaoming Fan, Shengtao Li, Ge Gao, Ming Gu; School of software/BNRist, Tsinghua University</li>
  <li>
<strong>PointToBIM</strong>: Mansour Mehranfar, Miguel Vega, Yuandong Pan, Saeed Mafipour, Florian Noichl, Fiona Collins; Technical University of Munich, Computational Modeling and Simulation</li>
  <li>
<strong>Box Detector</strong>: Yu Bai, Matthew and Jian Zhang; Tongji University</li>
</ul>

<hr>

<h2 id="questions">
<img class="emoji" title=":question:" alt=":question:" src="https://github.githubassets.com/images/icons/emoji/unicode/2753.png" height="20" width="20"> <strong>Questions</strong>
</h2>
<p>Contact the organisers at <strong><a href="mailto:cv4aec.3d@gmail.com">cv4aec.3d@gmail.com</a></strong></p>

<hr>
<h1 id="organizers"><strong>Organizers</strong></h1>
<h2 id="construction_worker-organizers">
<img class="emoji" title=":construction_worker:" alt=":construction_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f477.png" height="20" width="20"> <strong>Organizers</strong>
</h2>
<div class="container">
<figure>
    <a href="https://ir0.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/iroarmeni.jpg" alt="Iro Armeni"></a>
    <b><br><a href="https://ir0.github.io/">Iro Armeni</a>
    <br>Postdoctoral Researcher, CS &amp; CEE  <br> ETHZ</b>
</figure>

<figure>
    <a href="https://www.linkedin.com/in/erzhuo-ezra-che-40888137/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/erzhuoche.jpeg" alt="Erzhuo Che"></a>
    <b><br><a href="https://www.linkedin.com/in/erzhuo-ezra-che-40888137/">Erzhuo Che</a>
    <br>Assistant Professor, CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="https://web.stanford.edu/~fischer/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/martinfischer.jpg" alt="Martin Fischer"></a>
    <b><br><a href="https://web.stanford.edu/~fischer/">Martin Fischer</a>
    <br>Professor, CEE <br> Stanford</b>
</figure>

<figure>
    <a href="https://www.cs.sfu.ca/~furukawa/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/yasufurukawa.jpg" alt="Yasutaka Furukawa"></a>
    <b><br><a href="https://www.cs.sfu.ca/~furukawa/">Yasutaka Furukawa</a>
    <br>Associate Professor, CS <br> Simon Fraser</b>
</figure>

<figure>
    <a href="https://fcl.ethz.ch/people/Module-Lead/daniel-hall.html#:~:text=Dr%20Daniel%20Hall%20is%20co,Geomatic%20Engineering%20at%20ETH%20Z%C3%BCrich.">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/danielhall.jpeg" alt="Daniel Hall"></a>
    <b><br><a href="https://fcl.ethz.ch/people/Module-Lead/daniel-hall.html#:~:text=Dr%20Daniel%20Hall%20is%20co,Geomatic%20Engineering%20at%20ETH%20Z%C3%BCrich.">Daniel Hall</a>
    <br>Assistant Professor, CEE <br> ETHZ</b>
</figure>

<figure>
    <a href="https://research.engr.oregonstate.edu/geomatics/faculty-members">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/jaehoonjung.jpeg" alt="Jaehoon Jung"></a>
    <b><br><a href="https://research.engr.oregonstate.edu/geomatics/faculty-members">Jaehoon Jung</a>
    <br>Assistant Professor, CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="http://web.engr.oregonstate.edu/~lif/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/fuxinli.jpg" alt="Fuxin Li"></a>
    <b><br><a href="http://web.engr.oregonstate.edu/~lif/">Fuxin Li</a>
    <br>Associate Professor, CS <br> Oregon State</b>
</figure>

<figure>
    <a href="https://directory.forestry.oregonstate.edu/people/olsen-michael">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/michaelolsen.jpg" alt="Michael Olsen"></a>
    <b><br><a href="https://directory.forestry.oregonstate.edu/people/olsen-michael">Michael Olsen</a>
    <br>Associate Professor, CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="https://people.inf.ethz.ch/pomarc/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/marcpollefeys.jpeg" alt="Marc Pollefeys"></a>
    <b><br><a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
    <br>Professor, CS <br> ETHZ</b>
</figure>

<figure>
    <a href="https://cce.oregonstate.edu/turkan">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/yeldaturkan.jpg" alt="Yelda Turkan"></a>
    <b><br><a href="https://cce.oregonstate.edu/turkan">Yelda Turkan</a>
    <br>Assistant Professor, CEE <br> Oregon State</b>
</figure>



</div>


    </main>
  </body>
</html>
