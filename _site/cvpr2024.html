<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Computer Vision in the Built Environment</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Computer Vision in the Built Environment" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CV4AEC @ CVPR 2024 (18 June 2024)" />
<meta property="og:description" content="CV4AEC @ CVPR 2024 (18 June 2024)" />
<link rel="canonical" href="http://localhost:4000/cvpr2024.html" />
<meta property="og:url" content="http://localhost:4000/cvpr2024.html" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Computer Vision in the Built Environment" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"CV4AEC @ CVPR 2024 (18 June 2024)","headline":"Computer Vision in the Built Environment","url":"http://localhost:4000/cvpr2024.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<link rel="apple-touch-icon" sizes="180x180" href="assets/imgs/favicon/apple-touch-icon.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="assets/imgs/favicon/favicon-32x32.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="assets/imgs/favicon/favicon-16x16.jpg">
<link rel="manifest" href="assets/imgs/favicon/site.webmanifest">

<!--Set all links to open in new tab by default-->
<base target="_blank">

<!-- end custom head snippets -->


  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Computer Vision in the Built Environment</h1>
      <h2 class="project-tagline">CV4AEC @ CVPR 2024 (18 June 2024)</h2>
<!--      -->
      

      <a href="#news" class="btn" target="_self">News</a>
      <a href="#dates" class="btn" target="_self">Important Dates</a>
      <a href="#schedule" class="btn" target="_self">Schedule</a>
      <a href="#speakers" class="btn" target="_self">Keynote Speakers</a>
      <a href="#papers" class="btn" target="_self">Call for Papers</a>
      <a href="#challenge" class="btn" target="_self">Challenge</a>
      <a href="#organizers" class="btn" target="_self">Organizers</a>
      <br>
      <a href="/index" class="btn" target="_self" style="width:150px">Latest</a>
      <a href="/cvpr2024" class="btn" target="_self" style="width:150px">CVPR 2024</a>
      <a href="/cvpr2023" class="btn" target="_self" style="width:150px">CVPR 2023</a>
      <a href="/cvpr2022" class="btn" target="_self" style="width:150px">CVPR 2022</a>
      <a href="/cvpr2021" class="btn" target="_self" style="width:150px">CVPR 2021</a>
      
    </header>

    <script src="assets/js/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop({
      diameter: 56,
      backgroundColor: '#4196ff',
      textColor: '#fff',
    })</script>

    <main id="content" class="main-content" role="main">
      <p class="text-center"><img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> Welcome to the <strong>4<sup>th</sup> Workshop and Challenge on
Computer Vision In The Built Environment For The Design, Construction and Operation of Buildings</strong> organized at <img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> <a href="https://cvpr.thecvf.com/"><img class="rounded-rect" src="assets/imgs/cvpr2024.png" width="400px" alt="cvpr2024"></a></p>

<p>Building on the success of the previous three workshops, the 4th Workshop on Computer Vision in the Built Environment continues on connecting the domains of Architecture, Engineering, and Construction (AEC) with that of Computer Vision by establishing a common ground of interaction and identify shared research interests. Specifically, this workshop focuses on the as-is semantic status of built environments and the changes that take place within them over time. These topics will be presented from the dual lens of Computer Vision and AEC-FM, highlighting the limitations and bottlenecks related to developing applications for this specific domain. The objective is for attendees to learn more about AEC-FM and the variety of real-world problems that, if solved, could have a tangible impact on this multi trillion dollar industry as well as the overall quality of life across the globe.</p>

<p>The workshop will begin by establishing ways to capture the as-is status of a space with expert speakers from both the AEC and Computer Vision domains. Attendees will be then introduced to the type of information required for the spatiotemporal analysis of our built environment in AEC, with a focus on effective management, safety, and the role of users in this process. Following that, the topic of scene understanding from 3D and 4D reconstructions will be presented. Finally, to close the loop from understanding real-world built environments to designing built environments better and faster, the topic of scene synthesis at a geometric and semantic level will be presented. The importance of closing the loop for the AEC industry is paramount, especially when considering the design paradox. Architects are designing living spaces without any feedback from their previous designs. Learning to design using data from spaces that are already occupied and in-use, can provide designers with insights on what makes spaces appropriate for supporting the quality of life of the users.</p>

<p>To further establish connections between the two domains and identify what we can do right now and what is still hard to solve, we will host the <strong>4th International Scan-to-BIM competition</strong> targeted on acquiring the semantic as-is status of buildings given their 3D point clouds. Specifically, we will focus on the tasks of floorplan reconstruction and 3D building model reconstruction and present appropriate interdisciplinary metrics for solving them. The past two years we observed that a large gap remains before these problems can be considered solved and actually meet the needs of practitioners. We regard this workshop as the ideal environment for understanding the challenges and steps forward given that it provides convergence between the research and practical communities from multiple disciplines.</p>

<p>The workshop will therefore consist of two parts: invited <a href="#speakers" target="_self">keynote talks</a> and a  <a href="#challenge" target="_self">Scan-to-BIM challenge</a>.</p>

<hr>

<h2 id="news">
<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> <strong>News</strong>
</h2>
<ul>
  <li>
<strong>06 May 2024 —</strong> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> Presentation instructions sent to authors, details updated <a href="#instructions">here</a>.</li>
  <li>
<strong>25 Apr 2024 —</strong> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> Paper decision notification sent to authors.</li>
  <li>
<strong>18 Apr 2024 —</strong> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> Workshop schedule confirmed.</li>
  <li>
<strong>19 Mar 2024 —</strong> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> <strong>Afshin Dehghan</strong> confirmed as keynote speaker.</li>
  <li>
<strong>14 Feb 2024 —</strong> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> We are accepting paper submissions this year! Look at Important Dates and Call For Short Papers, <a href="https://cmt3.research.microsoft.com/CV4AEC2024/">CMT Submission Link</a>.</li>
  <li>
<strong>14 Feb 2024 —</strong> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> <strong>Caitlin Mueller</strong> confirmed as keynote speaker.</li>
  <li>
<strong>13 Feb  2024 —</strong> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> Tentative schedule and dates released.</li>
  <li>
<strong>26 Jan 2024 —</strong> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> <strong>Catherine De Wolf</strong> and <strong>Francis Engelmann</strong> confirmed as keynote speaker.</li>
  <li>
<strong>26 Jan 2024 —</strong> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> <strong>Derek Lichti</strong> and <strong>Yuanbo (Amber) Xiangli</strong> confirmed as keynote speaker.</li>
  <li>
<strong>26 Jan 2024 —</strong> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Website is live!</li>
</ul>

<hr>

<h2 id="dates">
<img class="emoji" title=":hourglass_flowing_sand:" alt=":hourglass_flowing_sand:" src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png" height="20" width="20"> <strong>Important Dates</strong>
</h2>
<blockquote>
  <p><strong>NOTE</strong>: The submission/release times are <strong>11:59:59 UTC</strong> on the specified date.</p>
</blockquote>

<p><strong><u>Short Paper Submission</u></strong></p>
<ul>
  <li>
<strong>1 Apr 2024 —</strong> Paper submission deadline</li>
  <li>
<strong>3 Apr 2024 —</strong> Papers distributed to reviewers</li>
  <li>
<strong>18 Apr 2024 —</strong> Review submission deadline</li>
  <li>
<strong>25 Apr 2024 —</strong> Notification to Authors</li>
</ul>

<p><strong><u>Challenge</u></strong></p>
<ul>
  <li>
<strong>8 Apr 2024 —</strong> Training + Validation + Testing data available for 2D &amp; 3D</li>
  <li>
<strong>9 Apr 2024 —</strong> Evaluation server <strong>open</strong> to evaluate test submissions</li>
  <li>
<strong><del>01 Jun</del> 05 Jun 2024 —</strong> Challenge Submission Deadline</li>
  <li>
<strong>07 Jun 2024 —</strong> Notification To Challenge Participants</li>
  <li>
<strong>18 Jun 2024 —</strong> CV4AEC Workshop @ CVPR 2024</li>
</ul>

<hr>

<h2 id="schedule">
<img class="emoji" title=":calendar:" alt=":calendar:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c6.png" height="20" width="20"> <strong>Schedule</strong>
</h2>
<p>The workshop took place on <strong>18 June 2024</strong> from <strong>09:00 - 17:00 PDT</strong>. The recording of our workshop for registered participants can be found on <a href="https://cvpr.thecvf.com/virtual/2024/workshop/23655">CVPR platform</a>.</p>

<blockquote>
  <p><strong>NOTE</strong>: Times are shown in <strong>Pacific Daylight Time</strong>. Please take this into account if joining the workshop virtually.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Time (PDT)</th>
      <th>Duration</th>
      <th>Event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>09:00 - 09:30</td>
      <td>30 mins</td>
      <td>Welcome &amp; Introduction</td>
    </tr>
    <tr>
      <td>09:30 - 10:00</td>
      <td>30 mins</td>
      <td>
<strong>Derek Lichti</strong> – Rigorous Object Precision Modelling for Reality Capture Viewpoint Planning</td>
    </tr>
    <tr>
      <td>10:00 - 10:30</td>
      <td>30 mins</td>
      <td>
<strong>Francis Engelmann</strong> – Foundation Models For 3D Scene Understanding</td>
    </tr>
    <tr>
      <td>10:30 - 11:00</td>
      <td>30 mins</td>
      <td>
<strong>Catherine De Wolf</strong> – Digital Transformation For Circular Construction</td>
    </tr>
    <tr>
      <td>11:00 - 11:15</td>
      <td>15 mins</td>
      <td><em>Coffee Break</em></td>
    </tr>
    <tr>
      <td>11:15 - 11:45</td>
      <td>30 mins</td>
      <td>
<strong>Afshin Dehghan</strong> – Apple LiDAR and Advanced Parametric Scene Representation: RoomPlan and Beyond</td>
    </tr>
    <tr>
      <td>11:45 - 12:45</td>
      <td>60 mins</td>
      <td>Oral Session</td>
    </tr>
    <tr>
      <td>12:45 - 14:15</td>
      <td>90 mins</td>
      <td>Poster Session (Arch Building Exhibit Hall, posters #40-49) &amp; <em>Lunch Break</em>
</td>
    </tr>
    <tr>
      <td>14:15 - 14:45</td>
      <td>30 mins</td>
      <td>Challenge Winner Presentations and Awards</td>
    </tr>
    <tr>
      <td>14:45 - 15:15</td>
      <td>30 mins</td>
      <td>
<strong>Caitlin Mueller</strong> – Designing With Data For A Sustainable Built Environment</td>
    </tr>
    <tr>
      <td>15:15 - 15:45</td>
      <td>30 mins</td>
      <td>
<strong>Yuanbo (Amber) Xiangli</strong> – Pack Built Environments into Neural Fields</td>
    </tr>
    <tr>
      <td>15:45 - 16:45</td>
      <td>60 mins</td>
      <td><em>Panel Discussion</em></td>
    </tr>
    <tr>
      <td>16:45 - 17:00</td>
      <td>15 mins</td>
      <td><em>Concluding Remarks</em></td>
    </tr>
  </tbody>
</table>

<hr>

<h2 id="speakers">
<img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> <strong>Keynote Speakers</strong>
</h2>

<div class="container">

<figure>
    <a href="https://www.afshindehghan.com/">
    <img class="img-author" src="assets/imgs/authors/cvpr2024/afshindehghan.jpg" alt="Afshin Dehghan"></a>
    <b><br><a href="https://www.afshindehghan.com/">Afshin Dehghan</a>
    <br>Machine Learning Manager <br>Apple</b>
</figure>

<figure>
    <a href="https://www.catherinedewolf.com/">
    <img class="img-author" src="assets/imgs/authors/cvpr2024/catherinedewolf.jpeg" alt="Catherine De Wolf"></a>
    <b><br><a href="https://www.catherinedewolf.com/">Catherine De Wolf</a>
    <br>Professor, CEE <br>ETH Zurich</b>
</figure>

<figure>
    <a href="https://francisengelmann.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2024/francisengelmann.jpeg" alt="Francis Engelmann"> </a>
    <b><br><a href="https://francisengelmann.github.io/">Francis Engelmann</a>
    <br>PostDoc, CS <br>ETH Zurich</b>
</figure>

<figure>
    <a href="https://www.geo-week.com/advisor/derek-lichti/">
    <img class="img-author" src="assets/imgs/authors/cvpr2024/dereklichti.jpeg" alt="Derek Lichti"></a>
    <b><br><a href="https://www.geo-week.com/advisor/derek-lichti/">Derek Lichti</a>
    <br>Professor, Geomatics <br>University of Calgary</b>
</figure>

<figure>
    <a href="https://cee.mit.edu/people_individual/caitlin-mueller/">
    <img class="img-author" src="assets/imgs/authors/cvpr2024/caitlinmueller.jpg" alt="Caitlin Mueller"></a>
    <b><br><a href="https://cee.mit.edu/people_individual/caitlin-mueller/">Caitlin Mueller</a>
    <br>Professor, CEE<br>MIT</b>
</figure>

<figure>
    <a href="https://kam1107.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2024/yuanboxiangli.jpeg" alt="Yuanbo (Amber) Xiangli"></a>
    <b><br><a href="https://kam1107.github.io/">Yuanbo (Amber) Xiangli</a>
    <br>Postdoc, CS <br>Cornell</b>
</figure>

</div>

<p><a href="https://www.afshindehghan.com/"><strong>Afshin Dehghan</strong></a>
is a Senior AI/ML Manager at Apple, where he leads a dynamic team dedicated to advancing multimodal perception and reasoning technologies. His team has made significant contributions to the development of several technologies across Apple, such as FaceID, the 2D &amp; 3D Always On perception engine on iPhones, and VisionPro. Additionally, his team has developed Apple’s 3D Parametric Scene Understanding technology, RoomPlan. This feature harnesses the power of liDAR technology in iPhones and iPads, allowing users to effortlessly create 3D floor-plans of their surroundings. More recently, his team has been focusing on multimodal applications that integrate visual, spatial, and contextual data, powered by large models to create more intuitive and powerful computing experiences. Afshin earned his PhD in 2016 under the supervision of Mubarak Shah at the University of Central Florida.</p>

<p><a href="https://www.catherinedewolf.com/"><strong>Catherine De Wolf</strong></a>
is assistant professor and director of the Chair of Circular Engineering for Architecture (CEA) at ETH Zurich. Her work explores digital innovations such as reality capture and AI to advance the built environment towards a circular economy. She has a dual background in civil engineering and architecture and  completed her PhD at MIT. She is on the steering committee of the Centre for Augmented Computational Design in Architecture, Engineering and Construction (Design++). Catherine is also a faculty at the AI Center, EMPA, the Future Cities Lab, and the National Centre of Competence in Research on Digital Fabrication (DFAB). Additionally, Catherine provides regular consultation on environmental impact assessments for both government entities like the European Commission and engineering design offices such as Arup. Throughout her career, she has gained international experience working at institutions like the University of Cambridge, TU Delft, EPFL, Nanjing University, Kuwait University, and the African Urban Metabolism Network. Her contributions to these projects were often made possible by securing multiple fellowships, including the Swiss Excellence, WBI World Excellence, and Marie Sklodowska-Curie Postdoctoral Fellowships.</p>

<p><a href="https://francisengelmann.github.io/"><strong>Francis Engelmann</strong></a>
is a PostDoc with Prof. Marc Pollefeys at ETH Zurich, and a visiting researcher at Google with Federico Tombari. His research interest lie at the intersection of computer vision and deep learning towards open-vocabulary 3D scene understanding with foundation models.
Francis is a Fellow of the ETH AI Center, the ELLIS Society, and the recipient of the ETHZ Career Seed Award.</p>

<p><a href="%22https://www.geo-week.com/advisor/derek-lichti/%22"><strong>Derek Lichti</strong></a> 
is a Professor of Civil Engineering and Computer Science &amp; Technology. Derek Lichti received his Bachelor’s degree in Survey Engineering from Toronto Metropolitan University in 1993 and MSc and PhD degrees in Geomatics Engineering from the University of Calgary in 1996 and 1999, respectively. He is currently Professor in the Department of Geomatics Engineering at the University of Calgary, which he joined in 2008 and served (2013-2018) as Department Head. He is currently ISPRS Congress Director and served (2013-2020) as Editor-in-Chief of the ISPRS Journal of Photogrammetry and Remote Sensing. His research program focuses on imaging metrology: precision 3D reality capture from imaging sensors, principally terrestrial laser scanners and digital cameras. It touches a wide range of applications including the documentation of at-risk cultural heritage sites, as-built modelling of industrial sites, wear and damage assessment in structures and industrial machinery, and dimensional control.</p>

<p><a href="https://cee.mit.edu/people_individual/caitlin-mueller/"><strong>Caitlin Mueller</strong></a> is an Associate Professor at MIT’s Department of Architecture and Department of Civil and Environmental Engineering, in the Building Technology Program, where she leads the Digital Structures research group.  She works at the creative interface of architecture, structural engineering, and computation, and focuses on new computational design and digital fabrication methods for innovative, high-performance buildings and structures that empower a more sustainable and equitable future. Mueller holds three degrees from MIT in Architecture, Computation, and Building Technology, and one from Stanford in Structural Engineering.  Her research is funded by federal agencies and industry partners, including the National Science Foundation, FEMA, the MIT Tata Center, the Dar Group, Holcim, Robert McNeel &amp; Associates, and Altair Engineering.  Mueller was awarded the ACADIA Innovative Research Award of Excellence by the Association for Computer Aided Design in Architecture in 2021 and the Diversity Achievement Award from the Association of Collegiate Schools of Architecture in 2022.</p>

<p><a href="https://kam1107.github.io/"><strong>Yuanbo (Amber) Xiangli</strong></a>
is a postdoc scholar at Cornell University, working with Prof. Noah Snavely. Prior to this, she did her Ph.D at Multimedia Lab, the Chinese University of Hong Kong, supervised by Prof. Dahua Lin. She received her Master degree from University of Oxford and Diploma from the University of Nottingham in Computer Science. Her research interests lie in 3D computer vision and generative modelling. She has been working on photorealistic and efficient large-scale 3D indoor/outdoor scenes rendering, manipulation and generation, leveraging diverse 2D/3D data sources, geographic and architectural information.</p>

<h2 id="accepted-papers">
<img class="emoji" title=":school:" alt=":school:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3eb.png" height="20" width="20"> <strong>Accepted Papers</strong>
</h2>
<ol>
  <li>
    <p><strong>Automatic Defurnishing of Indoor Panoramas</strong> [<a href="https://matterport.github.io/automatic-defurnishing-of-indoor-panoramas/">Project Page</a>] [<a href="https://arxiv.org/abs/2405.03682">arXiv</a>] <br>
Mira Slavcheva (Matterport), David Gausebeck (Matterport), Kevin Chen (Matterport), David Buchhofer (Matterport), Azwad Sabik (Matterport), Chen Ma (Matterport), Sachal Dhillon (Matterport), Olaf Brandt (Matterport), Alan Dolhasz (Matterport)</p>
  </li>
  <li>
    <p><strong>A Two-Stage Masked Autoencoder Based Network for Indoor Depth Completion</strong> [<a href="https://arxiv.org/pdf/2406.09792v1">arXiv</a>]<br>
Kailai Sun (National University of Singapore), Zhou Yang (BYD company)</p>
  </li>
  <li>
    <p><strong>ARCH2S: Dataset, Benchmark and Challenges for Learning Exterior Architectural Structures from Point Clouds</strong> [<a href="https://arxiv.org/pdf/2406.01337">arXiv</a>] <br>
Ka Lung Cheung (The Chinese University of Hong Kong), Chi Chung Lee (Hong Kong Metropolitan University)</p>
  </li>
  <li>
    <p><strong>Towards Automating the Retrospective Generation of BIM Models: A Unified Framework for 3D Semantic Reconstruction of the Built Environment</strong> [<a href="https://arxiv.org/pdf/2406.01480">arXiv</a>]<br>
Ka Lung Cheung (The Chinese University of Hong Kong), Chi Chung Lee (Hong Kong Metropolitan University)</p>
  </li>
  <li>
    <p><strong>Enhancing Polygonal Building Segmentation via Oriented Corners</strong> [<a href="https://arxiv.org/abs/2407.12256">arXiv</a>] <br>
Mohammad Moein Sheikholeslami (York University), Muhammad Kamran (York University), Andreas Wichmann (Jade University of Applied Sciences), Gunho Sohn (York University)</p>
  </li>
  <li>
    <p><strong>Window to Wall Ratio Detection using SegFormer</strong> [<a href="https://arxiv.org/abs/2406.02706">arXiv</a>] <br> 
Zoe De Simone (MIT), Sayandeep Biswas (MIT), Oscar Wu (MIT)</p>
  </li>
  <li>
    <p><strong>An Expeditious Spatial Mean Radiant Temperature Mapping Framework using Visual SLAM and Semantic Segmentation</strong> <br>
Wei Liang (CMU), Yiting Zhang (CMU), Ji Zhang (CMU), Erica Cochran Hameen (CMU)</p>
  </li>
  <li>
    <p><strong>Zero-Shot Construction Object Detection through Knowledge-based Feature Integrator</strong> <br>
Maryam Soleymani (Louisiana State University), Mahdi Bonyani (Louisiana State University), Chao Wang (Louisiana State University), Hyun-woo Jeon (Kyung Hee University)</p>
  </li>
  <li>
    <p><strong>Real-time Ergonomic Risk Assessment in Construction Sites through Spatiotemporal Graph Convolution Network</strong> <br>
Mahdi Bonyani (Louisiana State University), Maryam Soleymani (Louisiana State University), Chao Wang (Louisiana State University), Hyun-woo Jeon (Kyung Hee University)</p>
  </li>
  <li>
    <p><strong>BIM-Module for deep learning-based parametric IFC reconstruction</strong> <br>
Maarten Bassier (KU Leuven), Sam De Geyter (KU Leuven), Oscar Roman (Bruno Kessler Foundation), Roberto Battisti (Bruno Kessler Foundation), Heinder De Winter (KU Leuven), Gabriele Mazzacca (Bruno Kessler Foundation), Fabio Remondino (Bruno Kessler Foundation)</p>
  </li>
</ol>

<h2 id="papers">
<img class="emoji" title=":paperclip:" alt=":paperclip:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ce.png" height="20" width="20"> <strong>Call for Short Papers</strong>
</h2>
<p>This year we are inviting as part of the workshop the submission of short papers, which will not appear in the conference proceedings. Accepted papers will be presented in an oral session and will also have a spot in the poster session.</p>

<p>Short papers range from 3 to 4 pages without references. Submissions should otherwise follow the CVPR 2024 Author Kit provided by the main conference: <a href="https://github.com/cvpr-org/author-kit/releases">CVPR 2024 Auhtor Kit</a>. Papers that are not properly anonymized, or do not use the template, or have more than four pages (excluding references) will be considered for rejection without review.</p>

<p>Link to the submission system : <a href="https://cmt3.research.microsoft.com/CV4AEC2024/">CMT</a></p>

<p>Submissions should:</p>
<ul>
  <li>Introduce the topic and literature review, discussion on methodology, preliminary results.</li>
  <li>Motivate and place the topic in relation to the built environment and its specific application, including a comparison to current AEC practice</li>
  <li>Include a short discussion on considerations of practice, ethics, and organizations, as applicable.</li>
</ul>

<p><strong> Topics </strong>: Any topic that can be categorized as Computer Vision applications In The Built Environment For The Design, Construction and Operation of Buildings.</p>

<p>Including but not limited to:</p>
<ul>
  <li>Generative design</li>
  <li>Floorplan reconstruction</li>
  <li>Indoor layout synthesis</li>
  <li>Activity recognition (e.g., occupants in a building, workers in a construction site)</li>
  <li>Semantic 3D understanding (e.g., for renovation or construction)</li>
  <li>3D reconstruction (e.g., for renovation or construction)</li>
  <li>Material understanding</li>
  <li>Object/Scene localization</li>
  <li>Scene completion</li>
  <li>Change detection</li>
  <li>And more</li>
</ul>

<hr>

<h2 id="instructions">
<img class="emoji" title=":postbox:" alt=":postbox:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ee.png" height="20" width="20"> <strong>Instructions for Presentation and Poster</strong>
</h2>

<p>PRESENTATIONS:</p>
<ul>
  <li>The oral session will take place 11:45AM - 12:45PM in the workshop room.</li>
  <li>Each oral presentation will be 5 minutes with no Q&amp;A. Participants will be encouraged to ask questions to presenters during the poster session.</li>
  <li>We will use the organizers’ laptop to present all papers for smooth transition. Please upload a power point of your presentation using the following link the latest by <strong>June 15th, 23:59 PT</strong>: <a href="https://forms.gle/MoKAB3GK9VtJsoSCA">Google form</a>
</li>
  <li>We hope that you used the reviewers’ feedback to improve on minor revisions and that you will present an updated version during the workshop.</li>
</ul>

<p>POSTERS:</p>
<ul>
  <li>The poster session will take place together with the lunch break 12:45 - 2:15 PM</li>
  <li>All posters will be in the Arch Building Exhibit Hall and labeled per workshop.</li>
  <li>The maximum poster size is 4x8 and there is an onsite printing option (not mandatory). More information at: <a href="https://cvprworkshop.myprintdesk.net/DSF/SmartStore.aspx#!/Storefront">CVPR website</a>
</li>
  <li>DO NOT SUBMIT A WORKSHOP PAPER TO THE MAIN CONFERENCE PRINT SITE - IT WILL BE REJECTED.</li>
  <li>If you have a question, need to re-submit a file or need a receipt, please contact the provider directly atdsf.team@e-arc.com</li>
  <li>In the poster room, there will be tables but no power outlets.</li>
</ul>

<h2 id="challenge">
<img class="emoji" title=":checkered_flag:" alt=":checkered_flag:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c1.png" height="20" width="20"> <strong>Challenge</strong>
</h2>
<p>The workshop will host the 4th International Scan-to-BIM challenge. The challenge will include the following tasks:</p>

<p>I. 2D Floorplan Reconstruction <br>
II. 3D Building Model Reconstruction</p>

<p class="text-center"><strong>[<a href="https://github.com/GradientSpaces/cv4aec-challenge">GitHub</a>] — [<a href="https://codalab.lisn.upsaclay.fr/competitions/18523">2D Challenge</a>] — [<a href="https://codalab.lisn.upsaclay.fr/competitions/18526">3D Challenge</a>]</strong></p>

<h3 id="2d-floor-plan-reconstruction">2D Floor Plan Reconstruction</h3>

<p>The 2D Floorplan Reconstruction challenge contains a total of 31 buildings with multiple floors each and dozens of rooms on each floor. Of which, 20 buildings are designated as the training set, with a total of 49 point clouds. The validation and testing sets contain 5.5 buildings with 21 point clouds each. For each model, there is an aligned point cloud in LAZ format. For the training and validation sets, a corresponding floorplan aligned with the coordinate system of the point cloud is also provided. The challenge data and evaluation code can be found in this <strong><a href="https://github.com/GradientSpaces/cv4aec-challenge">Github repository</a></strong>. The submission should be made in the same JSON format as in the provided ground truth. We include metrics to evaluate the reconstruction of the walls, doors, and columns, as well as floor area in 2D :</p>

<ol>
  <li>
    <p><strong>Geometric Metrics</strong> <br>
 a. <em>IoU</em> of each room (a room is defined as a completely separated area with walls and doors). <br>
 b. <em>Accuracy of endpoints</em> : Precision/Recall at 3 different thresholds: 5cm, 10cm and 20cm, as well as the F-measure at each threshold will be evaluated in the coordinate system of the point cloud. The provided endpoints will be matched with the Hungarian algorithm to the point cloud, and every point that is within a certain threshold will be determined as a match. <br>
 c. <em>Orientation</em> For each matched line between the ground truth, we will compute the cosine similarity metric between them as the normalized dot product. If a line is not matched with ground truth, the cosine metric will be zero. Finally, the metric will be averaged over all the ground truth lines.</p>
  </li>
  <li>
    <p><strong>Topological Metrics</strong> <br>
 a. <em><a href="https://ieeexplore.ieee.org/document/5539950">Warping error</a></em> : The warping error will first warp the predicted floorplan to the ground truth with a homotopic deformation, and then compute the pixels that cannot match after the deformation. <br>
 b. <strong><em>Betti number error</em></strong> : The Betti number error will compare the Betti numbers between the prediction and the ground truth and output the absolute value of the difference.</p>
  </li>
</ol>

<h3 id="3d-building-model-reconstruction">3D Building Model Reconstruction</h3>

<p>The training data consists of 16 floors from 8 buildings. For each model, there is an aligned point cloud in LAZ format. The 3D building coordinates for walls, columns and doors are presented in 3 separate JSON files. We focus on the reconstruction of walls, columns, and doors. The challenge data and evaluation code can be found in this <strong><a href="https://github.com/GradientSpaces/cv4aec-challenge">Github repository</a></strong>. The submission should be made in the same JSON format as in the provided ground truth. We evaluate the submissions on a variety of metrics :</p>

<ol>
  <li>
<strong>3D IoU</strong> of the 3D bounding box of each wall</li>
  <li>
<strong>Accuracy of the endpoints</strong> : Precision/Recall at 3 different thresholds: 5cm, 10cm and 20cm, as well as F-measure will be evaluated in the coordinate system of the point cloud. The provided endpoints will be matched with the Hungarian algorithm to the point cloud, and every point that is within a certain threshold will be determined as a match. We evaluate per each of the three semantic types (i.e., wall, column, door).</li>
</ol>

<blockquote>
  <p>We would like to note that ALL the submissions <strong>need to be constructed automatically</strong>. Manual reconstructions are against the spirit of this challenge and will not be allowed.</p>
</blockquote>

<hr>

<h2 id="winners">
<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> <strong>Challenge Winners</strong>
</h2>
<h3 id="2d-floor-plan-reconstruction-1">2D Floor Plan Reconstruction</h3>

<table>
  <thead>
    <tr>
      <th>Team</th>
      <th>Precision (5cm)</th>
      <th>Precision (10cm)</th>
      <th>Precision (20cm)</th>
      <th>Recall(5cm)</th>
      <th>Recall(10cm)</th>
      <th>Recall (20cm)</th>
      <th>IoU</th>
      <th>Warping Error</th>
      <th>Betting Error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>HKU-iLab-2D</td>
      <td>0.031</td>
      <td>0.060</td>
      <td>0.093</td>
      <td>0.192</td>
      <td>0.347</td>
      <td>0.510</td>
      <td>0.563</td>
      <td>0.221</td>
      <td>1.308</td>
    </tr>
  </tbody>
</table>

<h3 id="3d-building-model-reconstruction-1">3D Building Model Reconstruction</h3>

<table>
  <thead>
    <tr>
      <th>Team</th>
      <th>Average IoU</th>
      <th>Columns IoU</th>
      <th>Doors IoU</th>
      <th>Walls IoU</th>
      <th>5cm Average F1</th>
      <th>10cm Average F1</th>
      <th>20cm Average F1</th>
      <th>10cm Columns F1</th>
      <th>10cm Doors F1</th>
      <th>10cm Walls F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>HKU-iLab-3D</td>
      <td>0.450</td>
      <td>0.475</td>
      <td>0.524</td>
      <td>0.362</td>
      <td>0.466</td>
      <td>0.567</td>
      <td>0.646</td>
      <td>0.600</td>
      <td>0.586</td>
      <td>0.528</td>
    </tr>
    <tr>
      <td>KUL-FBK</td>
      <td>0.270</td>
      <td>0.304</td>
      <td>0.231</td>
      <td>0.291</td>
      <td>0.327</td>
      <td>0.423</td>
      <td>0.500</td>
      <td>0.488</td>
      <td>0.376</td>
      <td>0.425</td>
    </tr>
    <tr>
      <td>HumanTech</td>
      <td>0.241</td>
      <td>0.357</td>
      <td>0.172</td>
      <td>0.243</td>
      <td>0.314</td>
      <td>0.444</td>
      <td>0.550</td>
      <td>0.643</td>
      <td>0.258</td>
      <td>0.515</td>
    </tr>
  </tbody>
</table>

<h3 id="teams"><strong>Teams</strong></h3>
<ul>
  <li>
<strong>HKU-iLab-2D</strong>: Longyong Wu, Ziqi Li, Meng Sun, Fan Xue; Department of Real Estate and Construction, The University of Hong Kong</li>
  <li>
<strong>HKU-iLab-3D</strong>: Siyuan Meng, Sou-Han Chen, Jiajia Wang, Fan Xue; Department of Real Estate and Construction, The University of Hong Kong</li>
  <li>
<strong>KUL-FBK</strong>: Maarten Bassier, Sam De Geyter, Heinder De Winter, Roberto Battisti, Oscar Roman; KU Leuven Department of Civil Engineering, Geomatics Section, Faculty of Engineering Technology, Ghent, Belgium &amp; Bruno Kessler Foundation (FBK), Trento, Italy</li>
  <li>
<strong>HumanTech</strong>: Mahdi Chamseddine, Fabian Kaufmann, Jason Rambach; DFKI and RPTU in Kaiserslautern, Germany</li>
</ul>

<hr>

<h2 id="questions">
<img class="emoji" title=":question:" alt=":question:" src="https://github.githubassets.com/images/icons/emoji/unicode/2753.png" height="20" width="20"> <strong>Questions</strong>
</h2>
<p>Contact the organisers at <strong><a href="mailto:cv4aec.3d@gmail.com">cv4aec.3d@gmail.com</a></strong></p>

<hr>
<h1 id="organizers"><strong>Organizers</strong></h1>
<h2 id="senior-organizers">
<img class="emoji" title=":construction_worker:" alt=":construction_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f477.png" height="20" width="20"> <strong>Senior Organizers</strong>
</h2>
<div class="container">
<figure>
    <a href="https://ir0.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/iroarmeni.jpg" alt="Iro Armeni"></a>
    <b><br><a href="https://ir0.github.io/">Iro Armeni</a>
    <br>Professor, CEE  <br> Stanford</b>
</figure>

<figure>
    <a href="https://www.linkedin.com/in/erzhuo-ezra-che-40888137/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/erzhuoche.jpeg" alt="Iro Armeni"></a>
    <b><br><a href="https://www.linkedin.com/in/erzhuo-ezra-che-40888137/">Erzhuo Che</a>
    <br>Assistant Professor (Senior Research), CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="https://web.stanford.edu/~fischer/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/martinfischer.jpg" alt="Martin Fischer"></a>
    <b><br><a href="https://web.stanford.edu/~fischer/">Martin Fischer</a>
    <br>Professor, CEE <br> Stanford</b>
</figure>

<figure>
    <a href="https://fcl.ethz.ch/people/Module-Lead/daniel-hall.html#:~:text=Dr%20Daniel%20Hall%20is%20co,Geomatic%20Engineering%20at%20ETH%20Z%C3%BCrich.">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/danielhall.jpeg" alt="Daniel Hall"></a>
    <b><br><a href="https://fcl.ethz.ch/people/Module-Lead/daniel-hall.html#:~:text=Dr%20Daniel%20Hall%20is%20co,Geomatic%20Engineering%20at%20ETH%20Z%C3%BCrich.">Daniel Hall</a>
    <br>Assistant Professor, CEE <br> ETHZ</b>
</figure>

<figure>
    <a href="https://research.engr.oregonstate.edu/geomatics/faculty-members">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/jaehoonjung.jpeg" alt="Jaehoon Jung"></a>
    <b><br><a href="https://research.engr.oregonstate.edu/geomatics/faculty-members">Jaehoon Jung</a>
    <br>Assistant Professor (Senior Research), CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="http://web.engr.oregonstate.edu/~lif/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/fuxinli.jpg" alt="Fuxin Li"></a>
    <b><br><a href="http://web.engr.oregonstate.edu/~lif/">Fuxin Li</a>
    <br>Associate Professor, CS <br> Oregon State</b>
</figure>

<figure>
    <a href="https://directory.forestry.oregonstate.edu/people/olsen-michael">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/michaelolsen.jpg" alt="Michael Olsen"></a>
    <b><br><a href="https://directory.forestry.oregonstate.edu/people/olsen-michael">Michael Olsen</a>
    <br>Associate Professor, CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="https://people.inf.ethz.ch/pomarc/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/marcpollefeys.jpeg" alt="Marc Pollefeys"></a>
    <b><br><a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
    <br>Professor, CS <br> ETHZ</b>
</figure>

<figure>
    <a href="https://cce.oregonstate.edu/turkan">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/yeldaturkan.jpg" alt="Yelda Turkan"></a>
    <b><br><a href="https://cce.oregonstate.edu/turkan">Yelda Turkan</a>
    <br>Assistant Professor, CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="https://www.linkedin.com/in/heidar-rastiveis/">
    <img class="img-author" src="assets/imgs/authors/cvpr2024/heidarrastiveis.jpg" alt="Heidar Rastiveis"></a>
    <b><br><a href="https://www.linkedin.com/in/heidar-rastiveis/">Heidar Rastiveis</a>
    <br>Assistant Professor (Senior Research), CEE <br> Oregon State</b>
</figure>


</div>

<h2 id="student-organizers">
<img class="emoji" title=":grimacing:" alt=":grimacing:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f62c.png" height="20" width="20"> <strong>Student Organizers</strong>
</h2>
<div class="container">
<figure>
    <a href="https://sayands.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/sayandebsarkar.jpg" alt="Sayan Deb Sarkar"></a>
    <b><br><a href="https://sayands.github.io/">Sayan Deb Sarkar</a>
    <br>MSc CS <br> ETHZ</b>
</figure>

<figure>
    <a href="https://antonskoltech.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/antonegorov.jpeg" alt="Anton Egorov"></a>
    <b><br><a href="https://antonskoltech.github.io/">Anton Egorov</a>
    <br>Research Assistant <br> Oregon State </b>
</figure>

<figure>
    <a href="https://www.linkedin.com/in/mohsen-arjmand-591388b9/">
    <img class="img-author" src="assets/imgs/authors/cvpr2024/mohsenarjmand.jpg" alt="Mohsen Arjmand"></a>
    <b><br><a href="https://www.linkedin.com/in/mohsen-arjmand-591388b9/">Mohsen Arjmand</a>
    <br>PhD, CEE <br> Oregon State</b>
</figure>

</div>


    </main>
  </body>
</html>
