<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Computer Vision for the Built World</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Computer Vision for the Built World" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CV4AEC @ CVPR 2026 (Date TBD)" />
<meta property="og:description" content="CV4AEC @ CVPR 2026 (Date TBD)" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Computer Vision for the Built World" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"CV4AEC @ CVPR 2026 (Date TBD)","headline":"Computer Vision for the Built World","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<link rel="apple-touch-icon" sizes="180x180" href="assets/imgs/favicon/apple-touch-icon.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="assets/imgs/favicon/favicon-32x32.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="assets/imgs/favicon/favicon-16x16.jpg">
<link rel="manifest" href="assets/imgs/favicon/site.webmanifest">

<!--Set all links to open in new tab by default-->
<base target="_blank">

<!-- end custom head snippets -->


  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Computer Vision for the Built World</h1>
      <h2 class="project-tagline">CV4AEC @ CVPR 2026 (Date TBD)</h2>
<!--      -->
      

      <a href="#news" class="btn" target="_self">News</a>
      <a href="#dates" class="btn" target="_self">Important Dates</a>
      <a href="#schedule" class="btn" target="_self">Schedule</a>
      <a href="#speakers" class="btn" target="_self">Keynote Speakers</a>
      <a href="#papers" class="btn" target="_self">Call for Papers</a>
      <a href="#challenge" class="btn" target="_self">Challenge</a>
      <a href="#organizers" class="btn" target="_self">Organizers</a>
      <br>
      <a href="/index" class="btn" target="_self" style="width:150px">Latest</a>
      <a href="/cvpr2024" class="btn" target="_self" style="width:150px">CVPR 2024</a>
      <a href="/cvpr2023" class="btn" target="_self" style="width:150px">CVPR 2023</a>
      <a href="/cvpr2022" class="btn" target="_self" style="width:150px">CVPR 2022</a>
      <a href="/cvpr2021" class="btn" target="_self" style="width:150px">CVPR 2021</a>
      
    </header>

    <script src="assets/js/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop({
      diameter: 56,
      backgroundColor: '#4196ff',
      textColor: '#fff',
    })</script>

    <main id="content" class="main-content" role="main">
      <p style="text-align: center; margin-bottom: 1.5em;"><img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> Welcome to the <strong>5<sup>th</sup> Workshop on
Computer Vision for the Built World</strong> organized at <img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"></p>

<p style="text-align: center; margin-top: 1.5em; margin-bottom: 1.5em;">
<a href="https://cvpr.thecvf.com/"><img class="rounded-rect" src="assets/imgs/cvpr2026.png" width="400px" alt="cvpr2026"></a>
</p>

<p>This workshop bridges the fields of Architecture, Engineering, and Construction (AEC) with Computer Vision by focusing on how construction, the most dynamic, data-rich, and physically grounded phase of the built environment, can inform the way we design. Construction sites continuously evolve in geometry, appearance, and topology, offering a uniquely challenging yet structured setting for advancing computer vision tasks such as spatiotemporal modeling, semantic reasoning, and multimodal understanding. At the same time, the representations learned from construction data — capturing how things are actually built, changed, and adapted — can feed back into design processes, informing more generative, data-driven, and sustainable decision-making.</p>

<p>The workshop explores how visual and multimodal data, including 3D scans, imagery, sensor streams, and language, can be used to model and predict the evolution of the built environment and inspire generative frameworks that translate these insights into actionable design knowledge. The goal is to connect bottom-up scene understanding with top-down design generation, effectively closing the loop between “as-built” and “as-designed.” Construction thus becomes not only an application domain but also an experimental testbed for foundational computer vision research — providing real-world complexity, scale, and temporal dynamics rarely captured in synthetic datasets.</p>

<p>Through paper submissions, keynote talks, and the <strong>Nothing Stands Still</strong> construction-data challenge, participants will engage with real-world, challenging testbeds that advance spatiotemporal 3D modeling, multimodal understanding, and semantic reasoning of evolving scenes in core vision research.</p>

<p>The workshop will consist of: invited <a href="#speakers" target="_self">keynote talks</a>, <a href="#papers" target="_self">paper submissions</a>, and the <a href="#challenge" target="_self">Nothing Stands Still Challenge</a>.</p>

<hr>

<h2 id="news">
<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> <strong>News</strong>
</h2>
<ul>
  <li>
<strong>16 Feb 2026 —</strong> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Website is live!</li>
</ul>

<hr>

<h2 id="topics">
<img class="emoji" title=":dart:" alt=":dart:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png" height="20" width="20"> <strong>Topics</strong>
</h2>

<ul>
  <li>
<strong>Learning from change:</strong> Vision-based modeling of evolving spaces/sites to capture temporal, geometric, and semantic changes across scales.</li>
  <li>
<strong>Generative and predictive modeling:</strong> Using construction-derived knowledge to propose design alternatives, predict project evolution, and support reuse and sustainability goals.</li>
  <li>
<strong>Multimodal scene understanding:</strong> Integrating 3D geometry, imagery, building sensors, and text-based project documentation for holistic understanding.</li>
  <li>
<strong>Design-process feedback:</strong> Translating as-built data into generative design prompts or constraints that make design more adaptive, data-informed, and context-aware.</li>
  <li>
<strong>Sustainability and circularity:</strong> Leveraging vision and generation to enable resource-conscious construction, renovation, and reuse of building components.</li>
  <li>
<strong>Benchmarking and evaluation:</strong> Introducing the Nothing Stands Still construction dataset and challenge on spatiotemporal 3D registration as a realistic testbed for evolving-scene understanding.</li>
</ul>

<hr>

<h2 id="dates">
<img class="emoji" title=":hourglass_flowing_sand:" alt=":hourglass_flowing_sand:" src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png" height="20" width="20"> <strong>Important Dates</strong>
</h2>
<blockquote>
  <p><strong>NOTE</strong>: The submission/release times are <strong>11:59:59 UTC</strong> on the specified date.</p>
</blockquote>

<p><strong><u>Archival Paper Submission (8 pages)</u></strong></p>
<ul>
  <li>
<strong>15 Jan 2026 —</strong> Submissions open</li>
  <li>
<strong>15 Mar 2026 —</strong> Submission deadline</li>
  <li>
<strong>20 Mar 2026 —</strong> Final decisions / Reviews sent</li>
  <li>
<strong>10 Apr 2026 —</strong> Camera ready deadline</li>
</ul>

<p><strong><u>Non-Archival Paper Submission (4 pages)</u></strong></p>
<ul>
  <li>
<strong>15 Jan 2026 —</strong> Submissions open</li>
  <li>
<strong>15 Apr 2026 —</strong> Submission deadline</li>
  <li>
<strong>1 May 2026 —</strong> Final decisions / Reviews sent</li>
</ul>

<p><strong><u>Nothing Stands Still Challenge</u></strong></p>
<ul>
  <li>
<strong>15 Jan 2026 —</strong> Dataset release &amp; Registration opens</li>
  <li>
<strong>15 Feb 2026 —</strong> Submission window opens (evaluation server live)</li>
  <li>
<strong>30 Apr 2026 —</strong> Challenge submission deadline</li>
  <li>
<strong>30 Apr – 4 May 2026 —</strong> Review &amp; Evaluation</li>
  <li>
<strong>5 May 2026 —</strong> Notification of challenge winners</li>
  <li>
<strong>TBD —</strong> CV4AEC Workshop @ CVPR 2026</li>
</ul>

<hr>

<h2 id="schedule">
<img class="emoji" title=":calendar:" alt=":calendar:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c6.png" height="20" width="20"> <strong>Schedule</strong>
</h2>
<p>The workshop will take place on a <strong>date TBD</strong> as a <strong>half-day in-person event (4 hours)</strong>.</p>

<blockquote>
  <p><strong>NOTE</strong>: The schedule is tentative. Exact times will be updated closer to the workshop date.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Duration</th>
      <th>Session</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0:00 – 0:10</td>
      <td>10 mins</td>
      <td>Welcome &amp; Introduction</td>
    </tr>
    <tr>
      <td>0:10 – 0:40</td>
      <td>30 mins</td>
      <td><strong>Keynote 1</strong></td>
    </tr>
    <tr>
      <td>0:40 – 1:10</td>
      <td>30 mins</td>
      <td><strong>Keynote 2</strong></td>
    </tr>
    <tr>
      <td>1:10 – 1:50</td>
      <td>40 mins</td>
      <td>Challenge Winners Session (10 min intro + 30 min presentations)</td>
    </tr>
    <tr>
      <td>1:50 – 2:30</td>
      <td>40 mins</td>
      <td><em>Poster Session + Coffee Break</em></td>
    </tr>
    <tr>
      <td>2:30 – 3:00</td>
      <td>30 mins</td>
      <td><strong>Keynote 3</strong></td>
    </tr>
    <tr>
      <td>3:00 – 3:30</td>
      <td>30 mins</td>
      <td><strong>Keynote 4</strong></td>
    </tr>
    <tr>
      <td>3:30 – 4:00</td>
      <td>30 mins</td>
      <td>Oral Presentations (Best papers)</td>
    </tr>
    <tr>
      <td>4:00</td>
      <td>5-10 mins</td>
      <td><em>Conclusion &amp; Closing Remarks</em></td>
    </tr>
  </tbody>
</table>

<hr>

<h2 id="speakers">
<img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> <strong>Keynote Speakers</strong>
</h2>

<div class="container">

<figure>
    <a href="https://www.cs.princeton.edu/~jiadeng/">
    <img class="img-author" src="assets/imgs/authors/cvpr2026/jiadeng.png" alt="Jia Deng"></a>
    <b><br><a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
    <br>Professor, CS <br>Princeton</b>
</figure>

<figure>
    <a href="https://engineering.nyu.edu/faculty/semiha-ergan">
    <img class="img-author" src="assets/imgs/authors/cvpr2026/semihaergan.png" alt="Semiha Ergan"></a>
    <b><br><a href="https://engineering.nyu.edu/faculty/semiha-ergan">Semiha Ergan</a>
    <br>Professor, CEE &amp; CSE <br>NYU</b>
</figure>

<figure>
    <a href="https://jianghz.me/">
    <img class="img-author" src="assets/imgs/authors/cvpr2026/huaizujiang.png" alt="Huaizu Jiang"></a>
    <b><br><a href="https://jianghz.me/">Huaizu Jiang</a>
    <br>Assistant Professor, CS <br>Northeastern</b>
</figure>

<figure>
    <a href="https://engineering.nyu.edu/faculty/debra-laefer">
    <img class="img-author" src="assets/imgs/authors/cvpr2026/debralaefer.png" alt="Debra Laefer"></a>
    <b><br><a href="https://engineering.nyu.edu/faculty/debra-laefer">Debra Laefer</a>
    <br>Professor, Urban Informatics <br>NYU</b>
</figure>

</div>

<p><a href="https://www.cs.princeton.edu/~jiadeng/"><strong>Jia Deng</strong></a>
is a Professor of Computer Science at Princeton University. His research focuses on computer vision and machine learning. He received his Ph.D. from Princeton University and his B.Eng. from Tsinghua University, both in computer science. He is a recipient of the Sloan Research Fellowship, the NSF CAREER award, the ONR Young Investigator award, an ICCV Marr Prize, a CVPR test-of-time award and two ECCV Best Paper Awards. His recent work demonstrates how procedural and generative approaches can create complex, realistic indoor scenes, bridging vision research and design.</p>

<p><a href="https://engineering.nyu.edu/faculty/semiha-ergan"><strong>Semiha Ergan</strong></a>
is a faculty member at the Department of Civil and Urban Engineering and Computer Science and Engineering at New York University, and an associated faculty at the Center for Urban Science and Progress (CUSP). With her background in civil engineering, AI and informatics, she leads the Building Informatics and Visualization Lab (biLAB) at NYU Tandon School of Engineering. BiLAB specializes in utilizing cutting-edge AI and sensing technologies to tackle challenges observed during the design, construction, and operation of facilities. The research team detects, quantifies, and visualizes patterns over time, leveraging data obtained from reality capture technologies (e.g., cameras, laser scanners) and embedded sensing. By exploiting the intersection of BIM, AI, robotics, and manufacturing processes, the lab enhances the scalability and efficiency of construction methods, particularly in modular construction contexts. Her work has been supported by DOE BTO, various programs of NSF, DARPA, and private organizations. Her achievements include NYU’s 2023 Distinguished Teacher Award, 2024 Inclusive Excellence Award, and 2015 DARPA Young Faculty Award.</p>

<p><a href="https://jianghz.me/"><strong>Huaizu Jiang</strong></a>
is an assistant professor in the Khoury College of Computer Sciences at Northeastern University. His research interests include computer vision, computational photography, machine learning, natural language processing, and artificial intelligence. Prior to joining Northeastern University, he was a Postdoc Researcher at Caltech and a Visiting Researcher at NVIDIA. He obtained his Ph.D. from UMass Amherst, advised by Prof. Erik Learned-Miller. His awards include the 2019-2020 NVIDIA Graduate Fellowship, 2019 Adobe Fellowship, and 2018 Outstanding Reviewer at IEEE/CVF CVPR. His recent work demonstrates how generative vision models can transform 2D building plans into realistic 3D environments, bridging perception and design.</p>

<p><a href="https://engineering.nyu.edu/faculty/debra-laefer"><strong>Debra Laefer</strong></a>
is a Full Professor of Urban Informatics, jointly appointed at New York University’s Center for Urban Science and Progress and its Department of Civil and Urban Engineering. With degrees from the University of Illinois Urbana-Champaign (MS, Ph.D.), NYU (MEng), and Columbia University (BS, BA), Prof. Laefer has a wide-ranging urban research background applied through the lens of remote sensing, civil engineering, and historic preservation. Her work often stands at the cross-roads of technology creation and community values such as devising technical solutions for protecting architecturally significant buildings from subsurface construction and subsurface utility network generation. Prof. Laefer and her Urban Modeling Group pioneer computationally efficient storage, querying, and visualization strategies that harness distributed computing-based solutions and bridge the gap between LiDAR and hyperspectral data availability and its usability for the engineering community.</p>

<hr>

<h2 id="papers">
<img class="emoji" title=":paperclip:" alt=":paperclip:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ce.png" height="20" width="20"> <strong>Call for Papers</strong>
</h2>

<p>We invite submissions exploring the intersection of Computer Vision and the Built Environment, focusing on applications that transform how we understand, model, and design buildings and construction sites. Construction sites and building lifecycles are dynamic, complex, and data-rich, providing an ideal real-world testbed for advancing computer vision methods while generating actionable insights for design, sustainability, and circular practices.</p>

<p>Both <strong>short non-archival papers (4 pages)</strong> and <strong>long archival papers (8 pages)</strong> are welcome. Submissions should:</p>
<ul>
  <li>Introduce the topic and review related literature.</li>
  <li>Present methodology, experiments, and results.</li>
  <li>Situate the work in the context of real-world AEC applications, highlighting potential improvements over current practice.</li>
  <li>Include discussion of practical, ethical, and organizational considerations, when relevant.</li>
</ul>

<p>The best two long archival papers and the best short non-archival paper will be presented during the workshop in the Oral Presentation session.</p>

<p>We also accept papers submitted to the main conference and accepted as long non-archival papers (8 pages). Please indicate on the manuscript submission that it is accepted at the main conference.</p>

<p><strong>Topics</strong> include but are not limited to:</p>
<ul>
  <li>Generative design and predictive modeling, including those informed by evolving data or other physical constraints.</li>
  <li>Floorplan reconstruction, indoor layout synthesis, and 3D scene generation.</li>
  <li>Activity recognition on construction sites or within occupied buildings.</li>
  <li>Semantic 3D/4D understanding for construction monitoring, renovation, or modular design related tasks.</li>
  <li>Large-scale 3D reconstruction of evolving sites and temporally dynamic environments.</li>
  <li>Material, component, and object understanding to support reuse and circular economy practices.</li>
  <li>Object and scene localization, mapping, scene completion, and change detection across evolving phases.</li>
  <li>Multimodal integration of 3D geometry, imagery, sensors, and text-based documentation to inform design and operational decisions.</li>
</ul>

<p>Each submission will be reviewed by at least two program committee members, chosen to provide complementary expertise across computer vision and AEC domains.</p>

<hr>

<h2 id="challenge">
<img class="emoji" title=":checkered_flag:" alt=":checkered_flag:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c1.png" height="20" width="20"> <strong>Nothing Stands Still Challenge</strong>
</h2>

<p>The workshop will host the <strong>2026 Nothing Stands Still (NSS) Dataset Challenge</strong>, introducing a unique real-world testbed for computer vision research. Previously run as part of a robotics conference workshop, the NSS challenge is now joining the computer vision community for the first time, reflecting its relevance for understanding complex, dynamic environments at scale. Full details from prior challenges are available at: <strong><a href="https://nothing-stands-still.com/challenge">nothing-stands-still.com/challenge</a></strong></p>

<p>The challenge focuses on <strong>spatiotemporal 3D point cloud registration</strong> of evolving construction sites, which feature dramatic changes in geometry, topology, and appearance over time. These dynamic environments make construction sites an ideal testbed for cutting-edge computer vision tasks, including scene reconstruction, semantic understanding, predictive modeling, and temporal reasoning. To expand the scope of the challenge, we aim to add semantic annotations, enabling participants to reason not only about geometry but also about functional elements, building components, and how they evolve over time.</p>

<!-- ![NSS Benchmark](assets/imgs/nss_benchmark.png) -->
<!-- *Figure 1. The Nothing Stands Still benchmark evaluates both pairwise and multi-way spatiotemporal 3D point cloud registration.* -->

<h3 id="evaluation">Evaluation</h3>

<ul>
  <li>
<strong>Spatiotemporal registration task:</strong> Submissions will be evaluated using a <strong>global RMSE metric</strong>, providing a standardized measure for comparing performance across methods.</li>
  <li>
<strong>Spatiotemporal semantic understanding task:</strong> Methods will be evaluated on standard segmentation metrics, as well as temporal counterparts.</li>
</ul>

<h3 id="challenge-timeline">Challenge Timeline</h3>

<table>
  <thead>
    <tr>
      <th>Milestone</th>
      <th>Date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Dataset Release &amp; Registration Opens</td>
      <td>January 15, 2026</td>
    </tr>
    <tr>
      <td>Submission Window Opens (evaluation server live)</td>
      <td>February 15, 2026</td>
    </tr>
    <tr>
      <td>Submission Deadline</td>
      <td>April 30, 2026</td>
    </tr>
    <tr>
      <td>Review &amp; Evaluation</td>
      <td>April 30 – May 4, 2026</td>
    </tr>
    <tr>
      <td>Notification of Challenge Winners</td>
      <td>May 5, 2026</td>
    </tr>
    <tr>
      <td>Workshop Presentation &amp; Awards</td>
      <td>TBD</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Ethical Note:</strong> All construction sites in the dataset are located in North America, which may limit the generalization of models trained on this data. Participants are encouraged to consider methods for robust and fair modeling across varied environments.</p>
</blockquote>

<p>By combining dynamic geometry, large-scale scene evolution, and future semantic reasoning, the NSS challenge offers the CV community a rigorous, high-impact platform to test algorithms in scenarios that closely mimic real-world challenges in construction, renovation, and modular building reuse — bridging the gap between technical innovation and tangible societal impact.</p>

<hr>

<h2 id="questions">
<img class="emoji" title=":question:" alt=":question:" src="https://github.githubassets.com/images/icons/emoji/unicode/2753.png" height="20" width="20"> <strong>Questions</strong>
</h2>
<p>Contact the organisers at <strong><a href="mailto:cv4aec.3d@gmail.com">cv4aec.3d@gmail.com</a></strong></p>

<hr>

<h1 id="organizers"><strong>Organizers</strong></h1>
<div class="container">

<figure>
    <a href="https://ir0.github.io/">
    <img class="img-author" src="assets/imgs/authors/organizers/iroarmeni.jpg" alt="Iro Armeni"></a>
    <b><br><a href="https://ir0.github.io/">Iro Armeni</a>
    <br>Assistant Professor <br>Stanford</b>
</figure>

<figure>
    <a href="https://sayands.github.io/">
    <img class="img-author" src="assets/imgs/authors/organizers/sayandebsarkar.jpg" alt="Sayan Deb Sarkar"></a>
    <b><br><a href="https://sayands.github.io/">Sayan Deb Sarkar</a>
    <br>PhD <br>Stanford</b>
</figure>

<figure>
    <a href="http://web.engr.oregonstate.edu/~lif/">
    <img class="img-author" src="assets/imgs/authors/organizers/fuxinli.jpg" alt="Fuxin Li"></a>
    <b><br><a href="http://web.engr.oregonstate.edu/~lif/">Fuxin Li</a>
    <br>Associate Professor <br>Oregon State</b>
</figure>

<figure>
    <a href="https://cce.oregonstate.edu/olsen">
    <img class="img-author" src="assets/imgs/authors/organizers/michaelolsen.jpg" alt="Michael Olsen"></a>
    <b><br><a href="https://cce.oregonstate.edu/olsen">Michael Olsen</a>
    <br>Dean's Professor <br>Oregon State</b>
</figure>

<figure>
    <a href="https://people.inf.ethz.ch/pomarc/">
    <img class="img-author" src="assets/imgs/authors/organizers/marcpollefeys.jpeg" alt="Marc Pollefeys"></a>
    <b><br><a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
    <br>Professor <br>ETH Zurich</b>
</figure>

<figure>
    <a href="#">
    <img class="img-author" src="assets/imgs/authors/organizers/emilysteiner.jpg" alt="Emily Steiner"></a>
    <b><br><a href="https://www.easteine.com/">Emily Steiner</a>
    <br>PhD <br>Stanford</b>
</figure>

<figure>
    <a href="https://taosun.io">
    <img class="img-author" src="assets/imgs/authors/organizers/taosun.jpg" alt="Tao Sun"></a>
    <b><br><a href="https://taosun.io">Tao Sun</a>
    <br>PhD <br>Stanford</b>
</figure>

<figure>
    <a href="https://engineering.oregonstate.edu/people/yelda-turkan">
    <img class="img-author" src="assets/imgs/authors/organizers/yeldaturkan.jpg" alt="Yelda Turkan"></a>
    <b><br><a href="https://engineering.oregonstate.edu/people/yelda-turkan">Yelda Turkan</a>
    <br>Associate Professor <br>Oregon State</b>
</figure>

</div>



    </main>
  </body>
</html>
